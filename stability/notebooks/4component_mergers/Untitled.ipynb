{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98057a73-9798-4181-905f-22d4ecec9ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "DTYPE = np.float64\n",
    "# import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "import os\n",
    "import pickle\n",
    "import gzip\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.insert(0, \"../../packages\")\n",
    "import flory_\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52157477-dbfa-4956-911c-be68483563de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flory Huggins Free Energy function\n",
    "def floryHuggins(phi:DTYPE, chi:np.array):\n",
    "    part_1 = np.sum(phi*np.log(phi))\n",
    "    part_2 = 0\n",
    "\n",
    "    for i in range(len(phi)):\n",
    "        for j in range(i+1, len(phi)):\n",
    "            part_2 += chi[i][j]*phi[i]*phi[j]\n",
    "\n",
    "    return part_1 + part_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aa506e9-8306-4048-a6a0-7570f1ddaa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Function for computing mergers of compartments\n",
    "# Returns the concentrations of components in the merged and the unmerged compartments\n",
    "# Stores them in phi_in_kmerged and phi_in_kunmerged respectively.\n",
    "    \n",
    "\n",
    "def mergers_4To2_combine3(concs:np.array, vols:np.array, chis, merged_compartments:list):\n",
    "    # Find the unmerged compartment(s)\n",
    "    expected = len(vols)*(len(vols)+1)//2\n",
    "    actual = np.sum(merged_compartments)\n",
    "    unmerged_compartment = expected - actual\n",
    "\n",
    "    # print(concs)\n",
    "    # print(vols)\n",
    "    # print(unmerged_compartment)\n",
    "\n",
    "    # Compute the merged volumes, stored in variable eta_merged\n",
    "    # subtract -1 from the merged_compartments idxs to maintain python idxing\n",
    "    eta_merged = 0\n",
    "    for compartment in merged_compartments:\n",
    "        eta_merged += vols[compartment-1]\n",
    "    # print(eta_merged)\n",
    "\n",
    "    # # Calculating the compositions of components in the merged compartment\n",
    "    # # Taking a simple weighted average\n",
    "    phi_1merged = (vols[merged_compartments[0]-1]*concs[0, merged_compartments[0]-1] + vols[merged_compartments[1]-1]*concs[0, merged_compartments[1]-1] + vols[merged_compartments[2]-1]*concs[0, merged_compartments[2]-1])/eta_merged\n",
    "    phi_2merged = (vols[merged_compartments[0]-1]*concs[1, merged_compartments[0]-1] + vols[merged_compartments[1]-1]*concs[1, merged_compartments[1]-1] + vols[merged_compartments[2]-1]*concs[1, merged_compartments[2]-1])/eta_merged\n",
    "    phi_3merged = (vols[merged_compartments[0]-1]*concs[2, merged_compartments[0]-1] + vols[merged_compartments[1]-1]*concs[2, merged_compartments[1]-1] + vols[merged_compartments[2]-1]*concs[2, merged_compartments[2]-1])/eta_merged\n",
    "    phi_4merged = 1 - phi_1merged - phi_2merged - phi_3merged\n",
    "    \n",
    "    # print(phi_1merged+phi_2merged+phi_3merged+phi_4merged)    \n",
    "\n",
    "    phi_in_kmerged = [phi_1merged, phi_2merged, phi_3merged, phi_4merged]\n",
    "    phi_in_kunmerged = [concs[0, unmerged_compartment-1], concs[1, unmerged_compartment-1], concs[2, unmerged_compartment-1], concs[3, unmerged_compartment-1]]\n",
    "    # print(phi_in_kmerged)\n",
    "    # print(phi_in_kunmerged)\n",
    "\n",
    "    # F_merged = eta_merged*floryHuggins(phi_in_kmerged, chis) + vols[unmerged_compartment-1]*floryHuggins(phi_in_kunmerged, chis)\n",
    "    # # print(F_merged)\n",
    "\n",
    "    return phi_in_kunmerged, phi_in_kmerged, eta_merged, unmerged_compartment\n",
    "\n",
    "def mergers_4To2_combine2And2(concs:np.array, vols:np.array, chis, merged_compartments_set1:list, merged_compartments_set2:list):\n",
    "    # Set 1\n",
    "    eta_merged_set1 = 0\n",
    "    for compartment in merged_compartments_set1:\n",
    "        eta_merged_set1 += vols[compartment-1]\n",
    "\n",
    "    phi_1merged_set1 = (vols[merged_compartments_set1[0]-1]*concs[0, merged_compartments_set1[0]-1] + vols[merged_compartments_set1[1]-1]*concs[0, merged_compartments_set1[1]-1])/eta_merged_set1\n",
    "    phi_2merged_set1 = (vols[merged_compartments_set1[0]-1]*concs[1, merged_compartments_set1[0]-1] + vols[merged_compartments_set1[1]-1]*concs[1, merged_compartments_set1[1]-1])/eta_merged_set1\n",
    "    phi_3merged_set1 = (vols[merged_compartments_set1[0]-1]*concs[2, merged_compartments_set1[0]-1] + vols[merged_compartments_set1[1]-1]*concs[2, merged_compartments_set1[1]-1])/eta_merged_set1\n",
    "    phi_4merged_set1 = 1 - phi_1merged_set1 - phi_2merged_set1 - phi_3merged_set1\n",
    "    # phi_4merged_set1 = (vols[merged_compartments_set1[0]-1]*concs[3, merged_compartments_set1[0]-1] + vols[merged_compartments_set1[1]-1]*concs[3, merged_compartments_set1[1]-1])/eta_merged_set1\n",
    "    phi_in_kset1 = [phi_1merged_set1, phi_2merged_set1, phi_3merged_set1, phi_4merged_set1]\n",
    "    # Set2\n",
    "    eta_merged_set2 = 0\n",
    "    for compartment in merged_compartments_set2:\n",
    "        eta_merged_set2 += vols[compartment-1]\n",
    "\n",
    "    phi_1merged_set2 = (vols[merged_compartments_set2[0]-1]*concs[0, merged_compartments_set2[0]-1] + vols[merged_compartments_set2[1]-1]*concs[0, merged_compartments_set2[1]-1])/eta_merged_set2\n",
    "    phi_2merged_set2 = (vols[merged_compartments_set2[0]-1]*concs[1, merged_compartments_set2[0]-1] + vols[merged_compartments_set2[1]-1]*concs[1, merged_compartments_set2[1]-1])/eta_merged_set2\n",
    "    phi_3merged_set2 = (vols[merged_compartments_set2[0]-1]*concs[2, merged_compartments_set2[0]-1] + vols[merged_compartments_set2[1]-1]*concs[2, merged_compartments_set2[1]-1])/eta_merged_set2\n",
    "    phi_4merged_set2 = 1 - phi_1merged_set2 - phi_2merged_set2 - phi_3merged_set2\n",
    "    phi_in_kset2 = [phi_1merged_set2, phi_2merged_set2, phi_3merged_set2, phi_4merged_set2]\n",
    "\n",
    "    # print(np.sum(phi_in_kset1), np.sum(phi_in_kset2), eta_merged_set1, eta_merged_set2)\n",
    "    return phi_in_kset1, phi_in_kset2, eta_merged_set1, eta_merged_set2\n",
    "\n",
    "def mergers_4To3_combine2(concs:np.array, vols:np.array, chis, merged_compartments:list):\n",
    "    eta_merged = 0\n",
    "    for compartment in merged_compartments:\n",
    "        eta_merged += vols[compartment-1]\n",
    "\n",
    "    phi_1merged = (vols[merged_compartments[0]-1]*concs[0, merged_compartments[0]-1]) + (vols[merged_compartments[1]-1]*concs[0, merged_compartments[1]-1]) \n",
    "    phi_2merged = (vols[merged_compartments[0]-1]*concs[1, merged_compartments[0]-1]) + (vols[merged_compartments[1]-1]*concs[1, merged_compartments[1]-1]) \n",
    "    phi_3merged = (vols[merged_compartments[0]-1]*concs[2, merged_compartments[0]-1]) + (vols[merged_compartments[1]-1]*concs[2, merged_compartments[1]-1]) \n",
    "    phi_4merged = 1 -phi_1merged - phi_2merged - phi_3merged\n",
    "\n",
    "    phi_in_kmerged = [phi_1merged, phi_2merged, phi_3merged, phi_4merged]\n",
    "    # print(eta_merged)\n",
    "    return phi_in_kmerged, eta_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4a7e253-f843-4124-a7de-89d570727a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_4To3_combine2(chis, concs, vols, steps):\n",
    "    merged_compartments_list = [np.array([1, 2], dtype=np.int64),\n",
    "                                np.array([1, 3], dtype=np.int64),\n",
    "                                np.array([1, 4], dtype=np.int64),\n",
    "                                np.array([2, 3], dtype=np.int64),\n",
    "                                np.array([2, 4], dtype=np.int64),\n",
    "                                np.array([3, 4], dtype=np.int64),\n",
    "                               ]\n",
    "    \n",
    "    for merged_compartments in merged_compartments_list:\n",
    "        # print(merged_compartments)\n",
    "\n",
    "        ##########################3\n",
    "        # --- | -\n",
    "        # merged | unmerged\n",
    "        phi_in_kmerged, eta_merged =  mergers_4To3_combine2(concs, vols, chis, merged_compartments)\n",
    "        unmerged  = np.setdiff1d( np.array([1, 2, 3 ,4], dtype=np.int64), merged_compartments)\n",
    "        \n",
    "        f_initial = eta_merged*floryHuggins(phi_in_kmerged, chis) + vols[unmerged[0]-1]*floryHuggins(concs[unmerged[0]-1], chis) + vols[unmerged[1]-1]*floryHuggins(concs[unmerged[1]-1], chis)\n",
    "    \n",
    "        phi_global_ = np.array([phi_in_kmerged, concs[unmerged[0]-1], concs[unmerged[1]-1]])\n",
    "        vols_ = np.array([eta_merged, vols[unmerged[0]-1], vols[unmerged[1]-1]])\n",
    "    \n",
    "        options = {\n",
    "                        \"num_part\": 3,\n",
    "                        \"progress\": False,\n",
    "                        \"max_steps\": steps,  # disable progress bar, allow more steps\n",
    "        }\n",
    "        \n",
    "        finder2 = flory_.CoexistingPhasesFinder(interaction, entropy, ensemble, **options)\n",
    "        # finder2.reinitialize_from_omegas(-np.log(np.transpose(phi_global_)), vols_)\n",
    "        finder2.reinitialize_from_phis(np.transpose(phi_global_), vols_)\n",
    "        phases = finder2.run(progress=False).get_clusters()\n",
    "        \n",
    "        vols_m =  phases.volumes/np.sum(phases.volumes)\n",
    "        fracs_m = phases.fractions\n",
    "        \n",
    "        f = []\n",
    "        for idx, _ in enumerate(vols_m):\n",
    "            f.append(vols_m[idx]*floryHuggins(fracs_m[idx], chis))\n",
    "        f_best = np.sum(f)\n",
    "        \n",
    "        output_filepath = f\"data/withFlory/phi_g{phi_global}/raw/X{X:.3f}/steps{steps}/mergers/4To3_combine2/{merged_compartments}/\"\n",
    "        output_filename = f\"initial_and_best.pkl\"\n",
    "        if not os.path.exists(output_filepath):\n",
    "            os.makedirs(output_filepath)\n",
    "        output_file = os.path.join(output_filepath, output_filename)\n",
    "        data_to_save = {\n",
    "            \"merged_compartments\": merged_compartments,\n",
    "            \"initial_guess\": [phi_global_, vols_],\n",
    "            \"F_initial\": f_initial,\n",
    "            \"best_location\": [fracs_m, vols_m],\n",
    "            \"F_best\": f_best,\n",
    "            \"metadata\": {\n",
    "                \"initial_guess\": \"stores the initial merged guesses for concs and volumes\",\n",
    "                \"F_initial\": \"stores the initial free energy for the guess\",\n",
    "                \"best_location\": \"stores flory's output for the local minima positions [phi_flory, eta_flory]\",\n",
    "                \"F_best\": \"stores the best free energy\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "        # print(data_to_save)\n",
    "        with gzip.open(output_file, \"wb\") as file:\n",
    "            pickle.dump(data_to_save, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        # print(f_initial, f_best)\n",
    "    # print()\n",
    "    # print()\n",
    "\n",
    "    \n",
    "def calculate_4To2_combine3(chis, concs, vols, steps):\n",
    "    merged_compartments_list = [np.array([1, 2, 3], dtype=np.int64), \n",
    "                                np.array([1, 2, 4], dtype=np.int64),\n",
    "                                np.array([1, 3, 4], dtype=np.int64),\n",
    "                                np.array([2, 3, 4], dtype=np.int64)]\n",
    "    \n",
    "    for merged_compartments in merged_compartments_list:\n",
    "        # print(merged_compartments)\n",
    "        phi_in_kunmerged, phi_in_kmerged, eta_merged, unmerged_compartment = mergers_4To2_combine3(concs, vols, chis, merged_compartments)\n",
    "        f_initial = eta_merged*floryHuggins(phi_in_kmerged, chis) + (1-eta_merged)*floryHuggins(phi_in_kunmerged, chis)\n",
    "        \n",
    "        phi_global_ = np.array([phi_in_kmerged, phi_in_kunmerged])\n",
    "        vols_ = np.array([eta_merged, 1-eta_merged])\n",
    "        \n",
    "        options = {\n",
    "                        \"num_part\": 2,\n",
    "                        \"progress\": False,\n",
    "                        \"max_steps\": steps,  # disable progress bar, allow more steps\n",
    "        }\n",
    "        \n",
    "        finder2 = flory_.CoexistingPhasesFinder(interaction, entropy, ensemble, **options)\n",
    "        # finder2.reinitialize_from_omegas(-np.log(np.transpose(phi_global_)), vols_)\n",
    "        finder2.reinitialize_from_phis(np.transpose(phi_global_), vols_)\n",
    "        phases = finder2.run(progress=False).get_clusters()\n",
    "    \n",
    "        vols_m =  phases.volumes/np.sum(phases.volumes)\n",
    "        fracs_m = phases.fractions\n",
    "    \n",
    "        f = []\n",
    "        for idx, _ in enumerate(vols_m):\n",
    "            f.append(vols_m[idx]*floryHuggins(fracs_m[idx], chis))\n",
    "        f_best = np.sum(f)\n",
    "    \n",
    "        output_filepath = f\"data/withFlory/phi_g{phi_global}/raw/X{X:.3f}/steps{steps}/mergers/4To2_combine3/{merged_compartments}/\"\n",
    "        output_filename = f\"initial_and_best.pkl\"\n",
    "        if not os.path.exists(output_filepath):\n",
    "            os.makedirs(output_filepath)\n",
    "        output_file = os.path.join(output_filepath, output_filename)\n",
    "        data_to_save = {\n",
    "            \"merged_compartments\": merged_compartments,\n",
    "            \"initial_guess\": [phi_global_, vols_],\n",
    "            \"F_initial\": f_initial,\n",
    "            \"best_location\": [fracs_m, vols_m],\n",
    "            \"F_best\": f_best,\n",
    "            \"metadata\": {\n",
    "                \"initial_guess\": \"stores the initial merged guesses [phi_merged, eta_merged]\",\n",
    "                \"F_initial\": \"stores the initial free energy for the guess\",\n",
    "                \"best_location\": \"stores flory's output for the local minima positions [phi_flory, eta_flory]\",\n",
    "                \"F_best\": \"stores the best free energy\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "        # print(data_to_save)\n",
    "        with gzip.open(output_file, \"wb\") as file:\n",
    "            pickle.dump(data_to_save, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        # print(f_initial, f_bfrom tqdm import tqdm\n",
    "    # print()\n",
    "    # print()\n",
    "\n",
    "\n",
    "def calculate_4To2_combine2And2(chis, concs, vols, steps):\n",
    "    merged_compartments_set1 = [np.array([1, 2], dtype=np.int64), \n",
    "                                np.array([1, 3], dtype=np.int64),\n",
    "                                np.array([1, 4], dtype=np.int64)]\n",
    "    \n",
    "    merged_compartments_set2 = [np.array([3, 4], dtype=np.int64), \n",
    "                                np.array([2, 4], dtype=np.int64),\n",
    "                                np.array([2, 3], dtype=np.int64)]\n",
    "    \n",
    "    for merged_compartments in (zip(merged_compartments_set1, merged_compartments_set2)):\n",
    "        # print(merged_compartments[0], merged_compartments[1])\n",
    "        phi_in_kset1, phi_in_kset2, eta_merged_set1, eta_merged_set2 = mergers_4To2_combine2And2(concs, vols, chis, merged_compartments[0], merged_compartments[1])\n",
    "        f_initial = eta_merged_set1*floryHuggins(phi_in_kset1, chis) + eta_merged_set2*floryHuggins(phi_in_kset2, chis)\n",
    "        # print(eta_merged_set1+eta_merged_set2)\n",
    "    \n",
    "        phi_global_ = np.array([phi_in_kset1, phi_in_kset2])\n",
    "        vols_ = np.array([eta_merged_set1, eta_merged_set2])\n",
    "    \n",
    "        options = {\n",
    "                        \"num_part\": 2,\n",
    "                        \"progress\": False,\n",
    "                        \"max_steps\": steps,  # disable progress bar, allow more steps\n",
    "        }\n",
    "        \n",
    "        finder2 = flory_.CoexistingPhasesFinder(interaction, entropy, ensemble, **options)\n",
    "        # finder2.reinitialize_from_omegas(-np.log(np.transpose(phi_global_)), vols_)\n",
    "        finder2.reinitialize_from_phis(np.transpose(phi_global_), vols_)\n",
    "        phases = finder2.run(progress=False).get_clusters()\n",
    "    \n",
    "        vols_m =  phases.volumes/np.sum(phases.volumes)\n",
    "        fracs_m = phases.fractions\n",
    "    \n",
    "        f = []\n",
    "        for idx, _ in enumerate(vols_m):\n",
    "            f.append(vols_m[idx]*floryHuggins(fracs_m[idx], chis))\n",
    "        f_best = np.sum(f)\n",
    "    \n",
    "        output_filepath = f\"data/withFlory/phi_g{phi_global}/raw/X{X:.3f}/steps{steps}/mergers/4To2_combine2And2/{merged_compartments[0]} {merged_compartments[1]}/\"\n",
    "        output_filename = f\"initial_and_best.pkl\"\n",
    "        if not os.path.exists(output_filepath):\n",
    "            os.makedirs(output_filepath)\n",
    "        output_file = os.path.join(output_filepath, output_filename)\n",
    "        data_to_save = {\n",
    "            \"merged_compartments\": merged_compartments,\n",
    "            \"initial_guess\": [phi_global_, vols_],\n",
    "            \"F_initial\": f_initial,\n",
    "            \"best_location\": [fracs_m, vols_m],\n",
    "            \"F_best\": f_best,\n",
    "            \"metadata\": {\n",
    "                \"initial_guess\": \"stores the initial merged guesses\",\n",
    "                \"F_initial\": \"stores the initial free energy for the guess\",\n",
    "                \"best_location\": \"stores flory's output for the local minima positions [phi_flory, eta_flory]\",\n",
    "                \"F_best\": \"stores the best free energy\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # print(data_to_save)\n",
    "        with gzip.open(output_file, \"wb\") as file:\n",
    "            pickle.dump(data_to_save, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        # print(f_initial, f_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d867857-d9a0-479d-8413-38f2469e3c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_globals = [np.array([0.1, 0.2, 0.3, 0.4], dtype=DTYPE),\n",
    "              np.array([0.1, 0.2, 0.4, 0.3], dtype=DTYPE),\n",
    "              np.array([0.1, 0.3, 0.2, 0.4], dtype=DTYPE),\n",
    "              np.array([0.1, 0.3, 0.4, 0.2], dtype=DTYPE),\n",
    "              np.array([0.1, 0.4, 0.2, 0.3], dtype=DTYPE),\n",
    "              np.array([0.1, 0.4, 0.3, 0.2], dtype=DTYPE),\n",
    "              np.array([0.2, 0.1, 0.3, 0.4], dtype=DTYPE),\n",
    "              np.array([0.2, 0.1, 0.4, 0.3], dtype=DTYPE),\n",
    "              np.array([0.2, 0.3, 0.1, 0.4], dtype=DTYPE),\n",
    "              np.array([0.2, 0.3, 0.4, 0.1], dtype=DTYPE),\n",
    "              np.array([0.2, 0.4, 0.3, 0.1], dtype=DTYPE),\n",
    "              np.array([0.2, 0.4, 0.1, 0.3], dtype=DTYPE),\n",
    "              np.array([0.3, 0.1, 0.2, 0.4], dtype=DTYPE),\n",
    "              np.array([0.3, 0.1, 0.4, 0.2], dtype=DTYPE),\n",
    "              np.array([0.3, 0.2, 0.1, 0.4], dtype=DTYPE),\n",
    "              np.array([0.3, 0.2, 0.4, 0.1], dtype=DTYPE),\n",
    "              np.array([0.3, 0.4, 0.1, 0.2], dtype=DTYPE),\n",
    "              np.array([0.3, 0.4, 0.2, 0.1], dtype=DTYPE),\n",
    "              np.array([0.4, 0.1, 0.2, 0.3], dtype=DTYPE),\n",
    "              np.array([0.4, 0.1, 0.3, 0.2], dtype=DTYPE),\n",
    "              np.array([0.4, 0.2, 0.1, 0.3], dtype=DTYPE),\n",
    "              np.array([0.4, 0.2, 0.3, 0.1], dtype=DTYPE),\n",
    "              np.array([0.4, 0.3, 0.1, 0.2], dtype=DTYPE),\n",
    "              np.array([0.4, 0.3, 0.2, 0.1], dtype=DTYPE)\n",
    "             ]\n",
    "\n",
    "Xs = np.arange(1, 10.1, 0.1)\n",
    "\n",
    "N_STEPS = [100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed379d6e-90ec-4e09-860f-214e23805291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1 0.2 0.3 0.4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 91/91 [03:01<00:00,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1 0.2 0.4 0.3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 91/91 [02:43<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1 0.3 0.2 0.4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 91/91 [02:56<00:00,  1.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1 0.3 0.4 0.2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 91/91 [02:47<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1 0.4 0.2 0.3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 91/91 [02:44<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1 0.4 0.3 0.2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 91/91 [02:49<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2 0.1 0.3 0.4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 91/91 [02:45<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2 0.1 0.4 0.3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 91/91 [02:58<00:00,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2 0.3 0.1 0.4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 91/91 [02:44<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2 0.3 0.4 0.1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 91/91 [02:53<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2 0.4 0.3 0.1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 91/91 [02:53<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2 0.4 0.1 0.3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 91/91 [02:50<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3 0.1 0.2 0.4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 91/91 [02:43<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3 0.1 0.4 0.2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 91/91 [02:54<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3 0.2 0.1 0.4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 91/91 [02:44<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3 0.2 0.4 0.1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 91/91 [02:34<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3 0.4 0.1 0.2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 91/91 [02:56<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3 0.4 0.2 0.1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 91/91 [02:40<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4 0.1 0.2 0.3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 91/91 [02:46<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4 0.1 0.3 0.2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 91/91 [02:37<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4 0.2 0.1 0.3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 91/91 [02:44<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4 0.2 0.3 0.1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 91/91 [02:52<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4 0.3 0.1 0.2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 91/91 [02:41<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4 0.3 0.2 0.1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 91/91 [02:55<00:00,  1.93s/it]\n"
     ]
    }
   ],
   "source": [
    "for phi_global in phi_globals:\n",
    "    print(phi_global)\n",
    "    for X in tqdm(Xs):\n",
    "        for steps in N_STEPS:\n",
    "            # print()\n",
    "            # print(X)\n",
    "            \n",
    "            chis = np.array([[0, 3, 3, 3+X],\n",
    "                             [3, 0, 3+X, 3],\n",
    "                             [3, 3+X, 0, 3],\n",
    "                             [3+X, 3, 3, 0]], dtype = DTYPE)\n",
    "        \n",
    "            n_components = 4\n",
    "            free_energy = flory_.free_energy.FloryHuggins(n_components, chis)\n",
    "            interaction = free_energy.interaction\n",
    "            entropy = free_energy.entropy\n",
    "            ensemble = flory_.CanonicalEnsemble(n_components, phi_global)\n",
    "            options = {\"num_part\": 32, \"progress\": False, \"max_steps\": 1000000}\n",
    "            \n",
    "            finder = flory_.CoexistingPhasesFinder(interaction, entropy, ensemble, **options)\n",
    "            phases = finder.run(progress=False)\n",
    "            \n",
    "            # Get the number of phases\n",
    "            vols = phases.get_clusters().volumes\n",
    "            concs = phases.get_clusters().fractions\n",
    "            \n",
    "            # Normalize volumes\n",
    "            vols = vols/np.sum(vols)\n",
    "            if concs.shape[0] == 4: # force 4 phase, else skip    \n",
    "                calculate_4To3_combine2(chis, concs, vols, steps)\n",
    "                calculate_4To2_combine3(chis, concs, vols, steps)\n",
    "                calculate_4To2_combine2And2(chis, concs, vols, steps)\n",
    "               \n",
    "            else:\n",
    "                print(f\"Skipped for {chis}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c2bc6a-aeca-4e1f-a549-0a6a08e220b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
